"""Simple inference CLI that demonstrates using a model_manifest to locate a model.\nThis is intentionally lightweight and does not download or load heavy models â€” adapt as needed.\n"""\nimport argparse\nimport json\n\n\ndef load_manifest(path="../ai_zsarpong_models/model_manifest.json"):\n    with open(path, "r") as f:\n        return json.load(f)\n\n\ndef infer(text, model_ref="gpt2"):\n    # Placeholder inference: echo back with metadata\n    return f"[INFER:{model_ref}] {text}"\n\n\ndef main():\n    parser = argparse.ArgumentParser(description="Inference CLI")\n    parser.add_argument("--text", type=str, required=True, help="Input text")\n    parser.add_argument("--manifest", type=str, default="../ai_zsarpong_models/model_manifest.json")\n    args = parser.parse_args()\n\n    manifest = load_manifest(args.manifest)\n    # choose first model for example\n    model_ref = manifest.get("models", [])[0].get("id")\n    out = infer(args.text, model_ref=model_ref)\n    print(out)\n\n\nif __name__ == "__main__":\n    main()\n